@inproceedings{YTBE,
author = {Covington, Paul and Adams, Jay and Sargin, Emre},
title = {Deep Neural Networks for YouTube Recommendations},
year = {2016},
isbn = {9781450340359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2959100.2959190},
doi = {10.1145/2959100.2959190},
abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation
systems in existence. In this paper, we describe the system at a high level and focus
on the dramatic performance improvements brought by deep learning. The paper is split
according to the classic two-stage information retrieval dichotomy: first, we detail
a deep candidate generation model and then describe a separate deep ranking model.
We also provide practical lessons and insights derived from designing, iterating and
maintaining a massive recommendation system with enormous user-facing impact.},
booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
pages = {191–198},
numpages = {8},
keywords = {deep learning, recommender system, scalability},
location = {Boston, Massachusetts, USA},
series = {RecSys '16}
}

@misc{ANOKHIN, title={Как я перестал бояться и научился любить нейронные сети}, url={https://habr.com/ru/company/odnoklassniki/blog/525974/}, author={Анохин, Николай}, year={2020}
} 
 
@inproceedings{AIRBNB,
author = {Haldar, Malay and Abdool, Mustafa and Ramanathan, Prashant and Xu, Tao and Yang, Shulin and Duan, Huizhong and Zhang, Qing and Barrow-Williams, Nick and Turnbull, Bradley C. and Collins, Brendan M. and Legrand, Thomas},
title = {Applying Deep Learning to Airbnb Search},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330658},
doi = {10.1145/3292500.3330658},
abstract = {The application to search ranking is one of the biggest machine learning success stories
at Airbnb. Much of the initial gains were driven by a gradient boosted decision tree
model. The gains, however, plateaued over time. This paper discusses the work done
in applying neural networks in an attempt to break out of that plateau. We present
our perspective not with the intention of pushing the frontier of new modeling techniques.
Instead, ours is a story of the elements we found useful in applying neural networks
to a real life product. Deep learning was steep learning for us. To other teams embarking
on similar journeys, we hope an account of our struggles and triumphs will provide
some useful pointers. Bon voyage!},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1927–1935},
numpages = {9},
keywords = {deep learning, search ranking, e-commerce},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{NCF,
author = {He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
title = {Neural Collaborative Filtering},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052569},
doi = {10.1145/3038912.3052569},
abstract = {In recent years, deep neural networks have yielded immense success on speech recognition,
computer vision and natural language processing. However, the exploration of deep
neural networks on recommender systems has received relatively less scrutiny. In this
work, we strive to develop techniques based on neural networks to tackle the key problem
in recommendation --- collaborative filtering --- on the basis of implicit feedback.Although
some recent work has employed deep learning for recommendation, they primarily used
it to model auxiliary information, such as textual descriptions of items and acoustic
features of musics. When it comes to model the key factor in collaborative filtering
--- the interaction between user and item features, they still resorted to matrix
factorization and applied an inner product on the latent features of users and items.By
replacing the inner product with a neural architecture that can learn an arbitrary
function from data, we present a general framework named NCF, short for Neural network-based
Collaborative Filtering. NCF is generic and can express and generalize matrix factorization
under its framework. To supercharge NCF modelling with non-linearities, we propose
to leverage a multi-layer perceptron to learn the user-item interaction function.
Extensive experiments on two real-world datasets show significant improvements of
our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows
that using deeper layers of neural networks offers better recommendation performance.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {173–182},
numpages = {10},
keywords = {matrix factorization, neural networks, collaborative filtering, deep learning, implicit feedback},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{BERT4Rec,
 author = {Sun, Fei and Liu, Jun and Wu, Jian and Pei, Changhua and Lin, Xiao and Ou, Wenwu and Jiang, Peng},
 title = {BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer},
 booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
 series = {CIKM '19},
 year = {2019},
 isbn = {978-1-4503-6976-3},
 location = {Beijing, China},
 pages = {1441--1450},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3357384.3357895},
 doi = {10.1145/3357384.3357895},
 acmid = {3357895},
 publisher = {ACM},
 address = {New York, NY, USA}
} 

@inproceedings{DSSM,
author = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
title = {Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2505665},
doi = {10.1145/2505515.2505665},
abstract = {Latent semantic models, such as LSA, intend to map a query to its relevant documents
at the semantic level where keyword-based matching often fails. In this study we strive
to develop a series of new latent semantic models with a deep structure that project
queries and documents into a common low-dimensional space where the relevance of a
document given a query is readily computed as the distance between them. The proposed
deep structured semantic models are discriminatively trained by maximizing the conditional
likelihood of the clicked documents given a query using the clickthrough data. To
make our models applicable to large-scale Web search applications, we also use a technique
called word hashing, which is shown to effectively scale up our semantic models to
handle large vocabularies which are common in such tasks. The new models are evaluated
on a Web document ranking task using a real-world data set. Results show that our
best model significantly outperforms other latent semantic models, which were considered
state-of-the-art in the performance prior to the work presented in this paper.},
booktitle = {Proceedings of the 22nd ACM International Conference on Information and Knowledge Management},
pages = {2333–2338},
numpages = {6},
keywords = {web search, semantic model, deep learning, clickthrough data},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@inproceedings{PINSAGE,
author = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219890},
doi = {10.1145/3219819.3219890},
abstract = {Recent advancements in deep neural networks for graph-structured data have led to
state-of-the-art performance on recommender system benchmarks. However, making these
methods practical and scalable to web-scale recommendation tasks with billions of
items and hundreds of millions of users remains an unsolved challenge. Here we describe
a large-scale deep recommendation engine that we developed and deployed at Pinterest.
We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines
efficient random walks and graph convolutions to generate embeddings of nodes (i.e.,
items) that incorporate both graph structure as well as node feature information.
Compared to prior GCN approaches, we develop a novel method based on highly efficient
random walks to structure the convolutions and design a novel training strategy that
relies on harder-and-harder training examples to improve robustness and convergence
of the model. We also develop an efficient MapReduce model inference algorithm to
generate embeddings using a trained model. Overall, we can train on and embed graphs
that are four orders of magnitude larger than typical GCN implementations. We show
how GCN embeddings can be used to make high-quality recommendations in various settings
at Pinterest, which has a massive underlying graph with 3 billion nodes representing
pins and boards, and 17 billion edges. According to offline metrics, user studies,
as well as A/B tests, our approach generates higher-quality recommendations than comparable
deep learning based systems. To our knowledge, this is by far the largest application
of deep graph embeddings to date and paves the way for a new generation of web-scale
recommender systems based on graph convolutional architectures.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {974–983},
numpages = {10},
keywords = {recommender systems, deep learning, scalability, graph convolutional networks},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{RANKING,
author = {Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
title = {Recommending What Video to Watch next: A Multitask Ranking System},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346997},
doi = {10.1145/3298689.3346997},
abstract = {In this paper, we introduce a large scale multi-objective ranking system for recommending
what video to watch next on an industrial video sharing platform. The system faces
many real-world challenges, including the presence of multiple competing ranking objectives,
as well as implicit selection biases in user feedback. To tackle these challenges,
we explored a variety of soft-parameter sharing techniques such as Multi-gate Mixture-of-Experts
so as to efficiently optimize for multiple ranking objectives. Additionally, we mitigated
the selection biases by adopting a Wide and Deep framework. We demonstrated that our
proposed techniques can lead to substantial improvements on recommendation quality
on one of the world's largest video sharing platforms.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {43–51},
numpages = {9},
keywords = {selection bias, recommendation and ranking, multitask learning},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{CB2CF,
author = {Barkan, Oren and Koenigstein, Noam and Yogev, Eylon and Katz, Ori},
title = {CB2CF: A Neural Multiview Content-to-Collaborative Filtering Model for Completely Cold Item Recommendations},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3347038},
doi = {10.1145/3298689.3347038},
abstract = {In Recommender Systems research, algorithms are often characterized as either Collaborative
Filtering (CF) or Content Based (CB). CF algorithms are trained using a dataset of
user preferences while CB algorithms are typically based on item profiles. These approaches
harness different data sources and therefore the resulting recommended items are generally
very different. This paper presents the CB2CF, a deep neural multiview model that
serves as a bridge from items content into their CF representations. CB2CF is a "real-world"
algorithm designed for Microsoft Store services that handle around a billion users
worldwide. CB2CF is demonstrated on movies and apps recommendations, where it is shown
to outperform an alternative CB model on completely cold items.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {228–236},
numpages = {9},
keywords = {multiview representation learning, cold item recommendations},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{DFN,
  title={Deep Feedback Network for Recommendation},
  author={Ruobing Xie and Chen Ling and Yalong Wang and Rui Wang and Feng Xia and Leyu Lin},
  booktitle={IJCAI},
  year={2020}
}

@inproceedings{DCN,
author = {Wang, Ruoxi and Shivanna, Rakesh and Cheng, Derek and Jain, Sagar and Lin, Dong and Hong, Lichan and Chi, Ed},
title = {DCN V2: Improved Deep and Cross Network and Practical Lessons for Web-Scale Learning to Rank Systems},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450078},
doi = {10.1145/3442381.3450078},
abstract = {Learning effective feature crosses is the key behind building recommender systems.
However, the sparse and large feature space requires exhaustive search to identify
effective crosses. Deep and Cross Network (DCN) was proposed to automatically and efficiently
learn bounded-degree predictive feature interactions. Unfortunately, in models that
serve web-scale traffic with billions of training examples, DCN showed limited expressiveness
in its cross network at learning more predictive feature interactions. Despite significant
research progress made, many deep learning models in production still rely on traditional
feed-forward neural networks to learn feature crosses inefficiently. In light of the
pros/cons of DCN and existing feature interaction learning approaches, we propose
an improved framework DCN-V2 to make DCN more practical in large-scale industrial
settings. In a comprehensive experimental study with extensive hyper-parameter search
and model tuning, we observed that DCN-V2 approaches outperform all the state-of-the-art
algorithms on popular benchmark datasets. The improved DCN-V2 is more expressive yet
remains cost efficient at feature interaction learning, especially when coupled with
a mixture of low-rank architecture. DCN-V2 is simple, can be easily adopted as building
blocks, and has delivered significant offline accuracy and online business metrics
gains across many web-scale learning to rank systems at Google. Our code and tutorial
are open-sourced as part of TensorFlow Recommenders (TFRS)1. },
booktitle = {Proceedings of the Web Conference 2021},
pages = {1785–1797},
numpages = {13},
keywords = {Neural Networks, CTR Prediction, Feature Crossing, Deep Learning},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@misc{ WD, title={Wide and Deep Learning: Better Together with TensorFlow}, url={https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html}, journal={Google AI blog}, author={ Cheng, Heng-Tze}, year={2016}, month={Jun}
}

@inproceedings{PROGRESS,
author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
title = {Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3347058},
doi = {10.1145/3298689.3347058},
abstract = {Deep learning techniques have become the method of choice for researchers working
on algorithmic aspects of recommender systems. With the strongly increased interest
in machine learning in general, it has, as a result, become difficult to keep track
of what represents the state-of-the-art at the moment, e.g., for top-n recommendation
tasks. At the same time, several recent publications point out problems in today's
research practice in applied machine learning, e.g., in terms of the reproducibility
of the results or the choice of the baselines when proposing new models.In this work,
we report the results of a systematic analysis of algorithmic proposals for top-n
recommendation tasks. Specifically, we considered 18 algorithms that were presented
at top-level research conferences in the last years. Only 7 of them could be reproduced
with reasonable effort. For these methods, it however turned out that 6 of them can
often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor
or graph-based techniques. The remaining one clearly outperformed the baselines but
did not consistently outperform a well-tuned non-neural linear ranking method. Overall,
our work sheds light on a number of potential problems in today's machine learning
scholarship and calls for improved scientific practices in this area.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {101–109},
numpages = {9},
keywords = {deep learning, reproducibility, evaluation, recommender systems},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{DALMANN,
author = {Dallmann, Alexander and Zoller, Daniel and Hotho, Andreas},
title = {A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3475943},
doi = {10.1145/3460231.3475943},
abstract = {At the present time, sequential item recommendation models are compared by calculating
metrics on a small item subset (target set) to speed up computation. The target set
contains the relevant item and a set of negative items that are sampled from the full
item set. Two well-known strategies to sample negative items are uniform random sampling
and sampling by popularity to better approximate the item frequency distribution in
the dataset. Most recently published papers on sequential item recommendation rely
on sampling by popularity to compare the evaluated models. However, recent work has
already shown that an evaluation with uniform random sampling may not be consistent
with the full ranking, that is, the model ranking obtained by evaluating a metric
using the full item set as target set, which raises the question whether the ranking
obtained by sampling by popularity is equal to the full ranking. In this work, we
re-evaluate current state-of-the-art sequential recommender models from the point
of view, whether these sampling strategies have an impact on the final ranking of
the models. We therefore train four recently proposed sequential recommendation models
on five widely known datasets. For each dataset and model, we employ three evaluation
strategies. First, we compute the full model ranking. Then we evaluate all models
on a target set sampled by the two different sampling strategies, uniform random sampling
and sampling by popularity with the commonly used target set size of 100, compute
the model ranking for each strategy and compare them with each other. Additionally,
we vary the size of the sampled target set. Overall, we find that both sampling strategies
can produce inconsistent rankings compared with the full ranking of the models. Furthermore,
both sampling by popularity and uniform random sampling do not consistently produce
the same ranking when compared over different sample sizes. Our results suggest that
like uniform random sampling, rankings obtained by sampling by popularity do not equal
the full ranking of recommender models and therefore both should be avoided in favor
of the full ranking when establishing state-of-the-art. },
booktitle = {Fifteenth ACM Conference on Recommender Systems},
pages = {505–514},
numpages = {10},
keywords = {Evaluation, Metrics, Sequential Item Recommendation, Sampled Metrics},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{SLIM,
  added-at = {2017-05-26T00:00:00.000+0200},
  author = {Ning, Xia and Karypis, George},
  biburl = {https://www.bibsonomy.org/bibtex/2b8ade7f72244d0efc68c20b0c989d7ab/dblp},
  booktitle = {ICDM},
  crossref = {conf/icdm/2011},
  editor = {Cook, Diane J. and Pei, Jian and Wang, Wei and Zaïane, Osmar R. and Wu, Xindong},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICDM.2011.134},
  interhash = {a65c6bedb17a01a61f5e3ce4ede2c58b},
  intrahash = {b8ade7f72244d0efc68c20b0c989d7ab},
  isbn = {978-0-7695-4408-3},
  keywords = {dblp},
  pages = {497-506},
  publisher = {IEEE Computer Society},
  timestamp = {2019-10-17T12:58:08.000+0200},
  title = {SLIM: Sparse Linear Methods for Top-N Recommender Systems.},
  url = {http://dblp.uni-trier.de/db/conf/icdm/icdm2011.html#NingK11},
  year = 2011
}

@inproceedings{EASE,
author = {Steck, Harald},
title = {Embarrassingly Shallow Autoencoders for Sparse Data},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313710},
doi = {10.1145/3308558.3313710},
abstract = {Combining simple elements from the literature, we define a linear model that is geared toward sparse data, in particular implicit feedback data for recommender systems. We show that its training objective has a closed-form solution, and discuss the resulting conceptual insights. Surprisingly, this simple model achieves better ranking accuracy than various state-of-the-art collaborative-filtering approaches, including deep non-linear models, on most of the publicly available data-sets used in our experiments.},
booktitle = {The World Wide Web Conference},
pages = {3251–3257},
numpages = {7},
keywords = {Autoencoder, Collaborative Filtering, Neighborhood Approach, Linear Regression, Recommender System, Closed-Form Solution},
location = {San Francisco, CA, USA},
series = {WWW '19}
}
